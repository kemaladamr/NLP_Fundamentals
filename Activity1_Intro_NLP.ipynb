{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activity1_Intro_NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNzSNtTPJy9r/EYPRmrIPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kemaladamr/NLP_Fundamentals/blob/main/Activity1_Intro_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXvDewVCcMK8",
        "outputId": "e57f9389-dfcb-425a-d8ba-1b4e4caa9689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[K     |████████████████████████████████| 622 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622382 sha256=cc4288c4bef68241d8561cc50c941d7f585df1bc7b7ebcd17c651ef627a09f1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/d4/37/8244101ad50b0f7d9bffd93ce58ed7991ee1753b290923934b\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install autocorrect\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')"
      ],
      "metadata": {
        "id": "N4OIq0c2fBYb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = open('file.txt', 'r').read()"
      ],
      "metadata": {
        "id": "ScUGOKfHdNsn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(sentence)\n",
        "print(words[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3qNjmoOdmtA",
        "outputId": "470b2b4a-5d29-4243-9a98-a5bb8eb027f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'this', 'book', 'authored', 'by', 'Sohom', 'Ghosh', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learnning', 'how', 'to', 'pracess', 'Natueral', 'Language', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_sentence = \"\"\n",
        "corrected_word_list = []\n",
        "for wd in words:\n",
        "    if wd not in string.punctuation:\n",
        "        wd_c = spell(wd)\n",
        "        if wd_c != wd:\n",
        "            print(wd + 'has been corrected to: ' + wd_c)\n",
        "            corrected_sentence = corrected_sentence + ' ' + wd_c\n",
        "            corrected_word_list.append(wd_c)\n",
        "        else:\n",
        "            corrected_sentence = corrected_sentence + ' ' + wd\n",
        "            corrected_word_list.append(wd)\n",
        "    else:\n",
        "        corrected_sentence = corrected_sentence + wd\n",
        "        corrected_word_list.append(wd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAzpA6qhduEV",
        "outputId": "61b8c209-0d21-4a4a-ea04-47dfbd17d9ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sohomhas been corrected to: Show\n",
            "Ghoshhas been corrected to: Ghost\n",
            "Dwighthas been corrected to: Right\n",
            "Gunninghas been corrected to: Running\n",
            "learnninghas been corrected to: learning\n",
            "pracesshas been corrected to: process\n",
            "Natueralhas been corrected to: Natural\n",
            "NLPhas been corrected to: LP\n",
            "NLPhas been corrected to: LP\n",
            "prajectshas been corrected to: projects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrected_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "YsDMZIRye3UJ",
        "outputId": "f4895c56-535e-4009-dbd0-5a348020b3bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' In this book authored by Show Ghost and Right Running, we shall learning how to process Natural Language and extract insights from it. The first four chapter will introduce you to the basics of LP. Later chapters will describe how to deal with complex LP projects. If you want to get early access of it, you should book your order now.'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corrected_word_list[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp8aX5fIhOE_",
        "outputId": "e34a50c6-3af0-4085-e68e-fe7dba2e0846"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'this', 'book', 'authored', 'by', 'Show', 'Ghost', 'and', 'Right', 'Running', ',', 'we', 'shall', 'learning', 'how', 'to', 'process', 'Natural', 'Language', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(corrected_word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxkcvwgfhWUY",
        "outputId": "ed47130a-4c3d-4171-dc43-527decfbdb61"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('In', 'IN'),\n",
              " ('this', 'DT'),\n",
              " ('book', 'NN'),\n",
              " ('authored', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('Show', 'NNP'),\n",
              " ('Ghost', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('Right', 'NNP'),\n",
              " ('Running', 'NNP'),\n",
              " (',', ','),\n",
              " ('we', 'PRP'),\n",
              " ('shall', 'MD'),\n",
              " ('learning', 'VB'),\n",
              " ('how', 'WRB'),\n",
              " ('to', 'TO'),\n",
              " ('process', 'VB'),\n",
              " ('Natural', 'NNP'),\n",
              " ('Language', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('extract', 'JJ'),\n",
              " ('insights', 'NNS'),\n",
              " ('from', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('first', 'JJ'),\n",
              " ('four', 'CD'),\n",
              " ('chapter', 'NN'),\n",
              " ('will', 'MD'),\n",
              " ('introduce', 'VB'),\n",
              " ('you', 'PRP'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'DT'),\n",
              " ('basics', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('LP', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Later', 'NNP'),\n",
              " ('chapters', 'NNS'),\n",
              " ('will', 'MD'),\n",
              " ('describe', 'VB'),\n",
              " ('how', 'WRB'),\n",
              " ('to', 'TO'),\n",
              " ('deal', 'VB'),\n",
              " ('with', 'IN'),\n",
              " ('complex', 'JJ'),\n",
              " ('LP', 'NNP'),\n",
              " ('projects', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('If', 'IN'),\n",
              " ('you', 'PRP'),\n",
              " ('want', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('early', 'JJ'),\n",
              " ('access', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " (',', ','),\n",
              " ('you', 'PRP'),\n",
              " ('should', 'MD'),\n",
              " ('book', 'NN'),\n",
              " ('your', 'PRP$'),\n",
              " ('order', 'NN'),\n",
              " ('now', 'RB'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "corrected_word_list_without_stopwords = []\n",
        "for wd in corrected_word_list:\n",
        "    if wd not in stop_words:\n",
        "        corrected_word_list_without_stopwords.append(wd)\n",
        "corrected_word_list_without_stopwords[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-jFNL-yhegT",
        "outputId": "829c0994-b458-4d0b-e8ff-13b1f1378e93"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'book',\n",
              " 'authored',\n",
              " 'Show',\n",
              " 'Ghost',\n",
              " 'Right',\n",
              " 'Running',\n",
              " ',',\n",
              " 'shall',\n",
              " 'learning',\n",
              " 'process',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'extract',\n",
              " 'insights',\n",
              " '.',\n",
              " 'The',\n",
              " 'first',\n",
              " 'four',\n",
              " 'chapter']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "corrected_word_list_without_stopwords_stemmed = []\n",
        "for wd in corrected_word_list_without_stopwords:\n",
        "    corrected_word_list_without_stopwords_stemmed.append(stemmer.stem(wd))\n",
        "corrected_word_list_without_stopwords_stemmed[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N7YlUBjiFOq",
        "outputId": "78da7810-6408-427f-86ed-dd9d72025ffa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'book',\n",
              " 'author',\n",
              " 'show',\n",
              " 'ghost',\n",
              " 'right',\n",
              " 'run',\n",
              " ',',\n",
              " 'shall',\n",
              " 'learn',\n",
              " 'process',\n",
              " 'natur',\n",
              " 'languag',\n",
              " 'extract',\n",
              " 'insight',\n",
              " '.',\n",
              " 'the',\n",
              " 'first',\n",
              " 'four',\n",
              " 'chapter']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "corrected_word_list_without_stopwords_lemmatized = []\n",
        "for wd in corrected_word_list_without_stopwords:\n",
        "    corrected_word_list_without_stopwords_lemmatized.append(lemmatizer.lemmatize(wd))\n",
        "corrected_word_list_without_stopwords_lemmatized[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VErfH7Sip-r",
        "outputId": "876b7958-d7bf-4f77-bf52-743b60fa34c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'book',\n",
              " 'authored',\n",
              " 'Show',\n",
              " 'Ghost',\n",
              " 'Right',\n",
              " 'Running',\n",
              " ',',\n",
              " 'shall',\n",
              " 'learning',\n",
              " 'process',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'extract',\n",
              " 'insight',\n",
              " '.',\n",
              " 'The',\n",
              " 'first',\n",
              " 'four',\n",
              " 'chapter']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(corrected_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSa1hiDJjOos",
        "outputId": "b2f76182-c468-46cf-9126-3023d4f9ba14"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' In this book authored by Show Ghost and Right Running, we shall learning how to process Natural Language and extract insights from it.',\n",
              " 'The first four chapter will introduce you to the basics of LP.',\n",
              " 'Later chapters will describe how to deal with complex LP projects.',\n",
              " 'If you want to get early access of it, you should book your order now.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hyq8v6qdjUHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}